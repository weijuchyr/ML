{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gl0OH3XGTXe1"
   },
   "source": [
    "# HW5 Skeleton Code\n",
    "Please note that this skeleton code is provided to help you with homework.\n",
    "Full description of each question can be found on HW5.pdf, so please read instruction of each question carefully. There might be some questions that is not presented in this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1QYqL7lvuvmK"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "awc_PHjUuvmR"
   },
   "source": [
    "## Q. Changing HTML Text to Plain Text\n",
    "\n",
    "The Python library <b>BeautifulSoup</b> is useful for dealing with html text. In order to use this library, you will need to install it first by running the following command: \n",
    " <b>conda install beautifulsoup4</b> \n",
    " in the terminal.\n",
    " <br> In the code, you can import it by running the following line: \n",
    "<br> \n",
    "  <b>from bs4 import BeautifulSoup </b>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JaHFZFOuuvmU"
   },
   "outputs": [],
   "source": [
    "  #Read our data file\n",
    "df_train = pd.read_csv(r'stack_stats_2023_train.csv') #Todo\n",
    "df_test = pd.read_csv(r'stack_stats_2023_test.csv') #Todo\n",
    "#df_train['Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "jzyC4JZpuvmW"
   },
   "outputs": [],
   "source": [
    "#Cleaning 'Body'\n",
    "#Change HTML Text to Plain text using get_text() function from BeautifulSoup\n",
    "#If you are not familiar with the apply method, please check discussion week 10 lecture and code.\n",
    "\n",
    "#Manually cleaned up newline tag \\n and tab tag \\t.\n",
    "df_train['Body'] = df_train['Body'].apply(lambda x: BeautifulSoup(x).get_text().replace('/n', '').replace('/t', '')) #Todo\n",
    "\n",
    "#Cleaning Tags\n",
    "#This would be somewhat similar to the above.\n",
    "#Manually cleaned up newline tag \\n and tab tag \\t.\n",
    "df_train['Title'] = df_train['Title'].apply(lambda x: BeautifulSoup(x).get_text().replace('/n', '').replace('/t', ''))\n",
    "\n",
    "\n",
    "#Todo: Repeat the same process for test dataset \n",
    "df_test['Body'] = df_test['Body'].apply(lambda x: BeautifulSoup(x).get_text().replace('/n', '').replace('/t', ''))\n",
    "df_test['Title'] = df_test['Title'].apply(lambda x: BeautifulSoup(x).get_text().replace('/n', '').replace('/t', ''))\n",
    "\n",
    "\n",
    "#df_train['Title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNS-3DkCuvmb"
   },
   "source": [
    "## Q. Basic Text Cleaning and Merging into a single Text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "031sBi5Duvmc"
   },
   "source": [
    "### Change to Lower Case, Remove puncuation, digits, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "t-YUYKlvuvmd"
   },
   "outputs": [],
   "source": [
    "#Change to Lowercase\n",
    "\n",
    "df_train[['Body','Title','Tags']] = df_train[['Body','Title','Tags']].applymap(lambda x: x.lower()) #Todo, do you see why we used applymap instead of apply in this case? \n",
    "df_test[['Body','Title','Tags']] = df_test[['Body','Title','Tags']].applymap(lambda x: x.lower()) #Todo\n",
    "\n",
    "#df_train['Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Bbcp_w_wuvme"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        why does the pytorch tutorial on dqn define st...\n",
       "1                        does random walking have a memory\n",
       "2        which statistic to report for repeated crossva...\n",
       "3        binary classification on imbalanced data  odd ...\n",
       "4        how to best summarize likert data to use as an...\n",
       "                               ...                        \n",
       "19242    is matrix factorization also going to work wit...\n",
       "19243    in reality there is almost always measurement ...\n",
       "19244    slight difference in the pmf of the poisson di...\n",
       "19245    how to prove that a function is 2increasing co...\n",
       "19246    how to test if this there is a genotypic effec...\n",
       "Name: Title, Length: 19247, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove Punctations \n",
    "from string import punctuation\n",
    "\n",
    "#You can get this function from our discussion session code. However, we leave it as a blank for a practice.\n",
    "def remove_punctuation(document):\n",
    "    \n",
    "    no_punct = ''.join([character for character in document if character not in punctuation])#Todo\n",
    "\n",
    "    return no_punct\n",
    "\n",
    "df_train['Body'] = df_train['Body'].apply(remove_punctuation)\n",
    "df_train['Title'] = df_train['Title'].apply(remove_punctuation)\n",
    "df_train['Tags'] = df_train['Tags'].apply(remove_punctuation)\n",
    "df_test['Body'] = df_test['Body'].apply(remove_punctuation)\n",
    "df_test['Title'] = df_test['Title'].apply(remove_punctuation)\n",
    "df_test['Tags'] = df_test['Tags'].apply(remove_punctuation)\n",
    "# df_train[['Body','Title','Tags']] = df_train[['Body','Title','Tags']].apply(remove_punctuation)#Todo \n",
    "# df_test[['Body','Title','Tags']] = df_test[['Body','Title','Tags']].apply(remove_punctuation)#Todo\n",
    "\n",
    "df_train['Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yNUiymSJuvmh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        why does the pytorch tutorial on dqn define st...\n",
       "1                        does random walking have a memory\n",
       "2        which statistic to report for repeated crossva...\n",
       "3        binary classification on imbalanced data  odd ...\n",
       "4        how to best summarize likert data to use as an...\n",
       "                               ...                        \n",
       "19242    is matrix factorization also going to work wit...\n",
       "19243    in reality there is almost always measurement ...\n",
       "19244    slight difference in the pmf of the poisson di...\n",
       "19245    how to prove that a function is increasing copula\n",
       "19246    how to test if this there is a genotypic effec...\n",
       "Name: Title, Length: 19247, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove Digits \n",
    "\n",
    "def remove_digit(document): \n",
    "    \n",
    "    no_digit = ''.join([character for character in document if not character.isdigit()])#Todo\n",
    "              \n",
    "    return no_digit\n",
    "\n",
    "df_train['Body'] = df_train['Body'].apply(remove_digit)\n",
    "df_train['Title'] = df_train['Title'].apply(remove_digit)\n",
    "df_train['Tags'] = df_train['Tags'].apply(remove_digit)\n",
    "df_test['Body'] = df_test['Body'].apply(remove_digit)\n",
    "df_test['Title'] = df_test['Title'].apply(remove_digit)\n",
    "df_test['Tags'] = df_test['Tags'].apply(remove_digit)\n",
    "\n",
    "# df_train[['Body','Title','Tags']] = df_train[['Body','Title','Tags']].apply(remove_digit)#Todo \n",
    "# df_test[['Body','Title','Tags']] = df_test[['Body','Title','Tags']].apply(remove_digit)#Todo\n",
    "\n",
    "df_train['Title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PAsCQQPQuvmi"
   },
   "source": [
    "### Tokenization and Remove Stopwords and do stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25821,
     "status": "ok",
     "timestamp": 1668123057199,
     "user": {
      "displayName": "Hyungki Im",
      "userId": "11059091126270115149"
     },
     "user_tz": 480
    },
    "id": "gOrVZxXQuvmj",
    "outputId": "07dd90e0-e5ca-4634-f43f-f379fe3a0787"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        [why, does, the, pytorch, tutorial, on, dqn, d...\n",
       "1                 [does, random, walking, have, a, memory]\n",
       "2        [which, statistic, to, report, for, repeated, ...\n",
       "3        [binary, classification, on, imbalanced, data,...\n",
       "4        [how, to, best, summarize, likert, data, to, u...\n",
       "                               ...                        \n",
       "19242    [is, matrix, factorization, also, going, to, w...\n",
       "19243    [in, reality, there, is, almost, always, measu...\n",
       "19244    [slight, difference, in, the, pmf, of, the, po...\n",
       "19245    [how, to, prove, that, a, function, is, increa...\n",
       "19246    [how, to, test, if, this, there, is, a, genoty...\n",
       "Name: Title, Length: 19247, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "df_train['Body'] = df_train['Body'].apply(word_tokenize)\n",
    "df_train['Title'] = df_train['Title'].apply(word_tokenize)\n",
    "df_train['Tags'] = df_train['Tags'].apply(word_tokenize)\n",
    "df_test['Body'] = df_test['Body'].apply(word_tokenize)\n",
    "df_test['Title'] = df_test['Title'].apply(word_tokenize)\n",
    "df_test['Tags'] = df_test['Tags'].apply(word_tokenize)\n",
    "# df_train[['Body','Title','Tags']] = df_train[['Body','Title','Tags']].apply(word_tokenize)#Todo \n",
    "# df_test[['Body','Title','Tags']] = df_test[['Body','Title','Tags']].apply(word_tokenize)#Todo\n",
    "\n",
    "df_train['Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2684,
     "status": "ok",
     "timestamp": 1668123197885,
     "user": {
      "displayName": "Hyungki Im",
      "userId": "11059091126270115149"
     },
     "user_tz": 480
    },
    "id": "82DJNdV1uvmj",
    "outputId": "8f8148ae-7551-43fc-d3d3-2bc8d8202d15"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        [pytorch, tutorial, dqn, define, state, differ...\n",
       "1                                [random, walking, memory]\n",
       "2           [statistic, report, repeated, crossvalidation]\n",
       "3        [binary, classification, imbalanced, data, odd...\n",
       "4        [best, summarize, likert, data, use, independe...\n",
       "                               ...                        \n",
       "19242    [matrix, factorization, also, going, work, one...\n",
       "19243    [reality, almost, always, measurement, error, ...\n",
       "19244     [slight, difference, pmf, poisson, distribution]\n",
       "19245                [prove, function, increasing, copula]\n",
       "19246              [test, genotypic, effect, time, effect]\n",
       "Name: Title, Length: 19247, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove Stopwords\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(document):\n",
    "    \n",
    "    words = [word for word in document if not word in stop_words]#Todo\n",
    "    \n",
    "    return words\n",
    "\n",
    "df_train['Body'] = df_train['Body'].apply(remove_stopwords)\n",
    "df_train['Title'] = df_train['Title'].apply(remove_stopwords)\n",
    "df_train['Tags'] = df_train['Tags'].apply(remove_stopwords)\n",
    "df_test['Body'] = df_test['Body'].apply(remove_stopwords)\n",
    "df_test['Title'] = df_test['Title'].apply(remove_stopwords)\n",
    "df_test['Tags'] = df_test['Tags'].apply(remove_stopwords)\n",
    "# df_train[['Body','Title','Tags']] = df_train[['Body','Title','Tags']].apply(remove_stopwords)#Todo\n",
    "# df_test[['Body','Title','Tags']] = df_test[['Body','Title','Tags']].apply(remove_stopwords)#Todo\n",
    "\n",
    "df_train['Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "54S-irnRuvmk"
   },
   "outputs": [],
   "source": [
    "#We use porter stemming \n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def stemmer(document):\n",
    "    \n",
    "    stemmed_document = [porter.stem(word) for word in document] #Todo\n",
    "    \n",
    "    return stemmed_document\n",
    "\n",
    "df_train['Body'] = df_train['Body'].apply(stemmer)\n",
    "df_train['Title'] = df_train['Title'].apply(stemmer)\n",
    "df_train['Tags'] = df_train['Tags'].apply(stemmer)\n",
    "df_test['Body'] = df_test['Body'].apply(stemmer)\n",
    "df_test['Title'] = df_test['Title'].apply(stemmer)\n",
    "df_test['Tags'] = df_test['Tags'].apply(stemmer)\n",
    "# df_train[['Body','Title','Tags']] = df_train[['Body','Title','Tags']].apply(stemmer) #Todo\n",
    "# df_test[['Body','Title','Tags']] = df_test[['Body','Title','Tags']].apply(stemmer) #Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2sfBcaMLTok"
   },
   "source": [
    "## Let's Check our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "r6V33S7pLS76"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Score</th>\n",
       "      <th>Body</th>\n",
       "      <th>Title</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>502641</td>\n",
       "      <td>1</td>\n",
       "      <td>[im, master, student, eec, work, way, toward, ...</td>\n",
       "      <td>[pytorch, tutori, dqn, defin, state, differ]</td>\n",
       "      <td>[machinelearningreinforcementlearningqlearn]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>477291</td>\n",
       "      <td>1</td>\n",
       "      <td>[know, good, question, found, answer, anywher,...</td>\n",
       "      <td>[random, walk, memori]</td>\n",
       "      <td>[probabilitylawoflargenumb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>448489</td>\n",
       "      <td>4</td>\n",
       "      <td>[time, repeat, fold, crossvalid, want, report,...</td>\n",
       "      <td>[statist, report, repeat, crossvalid]</td>\n",
       "      <td>[crossvalid]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>487075</td>\n",
       "      <td>0</td>\n",
       "      <td>[dataset, mm, record, around, featur, class, i...</td>\n",
       "      <td>[binari, classif, imbalanc, data, odd, calibr,...</td>\n",
       "      <td>[unbalancedclassescalibr]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>481670</td>\n",
       "      <td>2</td>\n",
       "      <td>[want, run, regress, one, explanatori, variabl...</td>\n",
       "      <td>[best, summar, likert, data, use, independ, va...</td>\n",
       "      <td>[multipleregressionmissingdatalikertitemrespon...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id  Score                                               Body  \\\n",
       "0  502641      1  [im, master, student, eec, work, way, toward, ...   \n",
       "1  477291      1  [know, good, question, found, answer, anywher,...   \n",
       "2  448489      4  [time, repeat, fold, crossvalid, want, report,...   \n",
       "3  487075      0  [dataset, mm, record, around, featur, class, i...   \n",
       "4  481670      2  [want, run, regress, one, explanatori, variabl...   \n",
       "\n",
       "                                               Title  \\\n",
       "0       [pytorch, tutori, dqn, defin, state, differ]   \n",
       "1                             [random, walk, memori]   \n",
       "2              [statist, report, repeat, crossvalid]   \n",
       "3  [binari, classif, imbalanc, data, odd, calibr,...   \n",
       "4  [best, summar, likert, data, use, independ, va...   \n",
       "\n",
       "                                                Tags  \n",
       "0       [machinelearningreinforcementlearningqlearn]  \n",
       "1                        [probabilitylawoflargenumb]  \n",
       "2                                       [crossvalid]  \n",
       "3                          [unbalancedclassescalibr]  \n",
       "4  [multipleregressionmissingdatalikertitemrespon...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hFgV2VL0uvml"
   },
   "source": [
    "### Q. Treat Three text data independently and merge into one column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "VCB-voc_uvmm"
   },
   "outputs": [],
   "source": [
    "#Treat Three types of data independently\n",
    "#let's define functions that will help this operation\n",
    "def add_body(document):\n",
    "    \n",
    "    string = '_body'\n",
    "    added_document = [x + string for x in document]\n",
    "  \n",
    "    return added_document\n",
    "\n",
    "def add_title(document):\n",
    "    \n",
    "    string = '_title'\n",
    "    added_document = [x + string for x in document]\n",
    "  \n",
    "    return added_document\n",
    "\n",
    "def add_tags(document):\n",
    "    \n",
    "    string = '_tags'\n",
    "    added_document = [x + string for x in document]\n",
    "  \n",
    "    return added_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "UWy0BGVHuvmm"
   },
   "outputs": [],
   "source": [
    "df_train['Body'] = df_train['Body'].apply(lambda x: add_body(x))\n",
    "df_train['Title'] = df_train['Title'].apply(lambda x: add_title(x))\n",
    "df_train['Tags'] = df_train['Tags'].apply(lambda x: add_tags(x))\n",
    "\n",
    "df_test['Body'] = df_test['Body'].apply(lambda x: add_body(x))\n",
    "df_test['Title'] = df_test['Title'].apply(lambda x: add_title(x))\n",
    "df_test['Tags'] = df_test['Tags'].apply(lambda x: add_tags(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [write_body, report_body, whether_body, enviro...\n",
      "1       [calcul_body, standard_body, error_body, sigma...\n",
      "2       [im_body, deal_body, dataset_body, input_body,...\n",
      "3       [number_body, place_body, exampl_body, ive_bod...\n",
      "4       [build_body, experiment_body, design_body, var...\n",
      "                              ...                        \n",
      "8244    [estim_body, theori_body, sampl_body, help_bod...\n",
      "8245    [dataset_body, repres_body, matrix_body, repea...\n",
      "8246    [would_body, like_body, know_body, uncertainti...\n",
      "8247    [peak_body, valu_body, normal_body, distribut_...\n",
      "8248    [question_body, refer_body, book_body, nonpara...\n",
      "Name: text, Length: 8249, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Now we need to merge all those 3 columns into a single column. Implement this below.\n",
    "df_train['text'] = df_train['Body'] + df_train['Title'] + df_train['Tags']#Todo\n",
    "df_test['text'] = df_test['Body'] + df_test['Title'] + df_test['Tags']#Todo\n",
    "\n",
    "print(df_test['text'])\n",
    "# len(df_test['text'].axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hq2PYhnmLhDi"
   },
   "source": [
    "## Let's check our DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "rtYP_Lu3LjIb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Score</th>\n",
       "      <th>Body</th>\n",
       "      <th>Title</th>\n",
       "      <th>Tags</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>502641</td>\n",
       "      <td>1</td>\n",
       "      <td>[im_body, master_body, student_body, eec_body,...</td>\n",
       "      <td>[pytorch_title, tutori_title, dqn_title, defin...</td>\n",
       "      <td>[machinelearningreinforcementlearningqlearn_tags]</td>\n",
       "      <td>[im_body, master_body, student_body, eec_body,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>477291</td>\n",
       "      <td>1</td>\n",
       "      <td>[know_body, good_body, question_body, found_bo...</td>\n",
       "      <td>[random_title, walk_title, memori_title]</td>\n",
       "      <td>[probabilitylawoflargenumb_tags]</td>\n",
       "      <td>[know_body, good_body, question_body, found_bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>448489</td>\n",
       "      <td>4</td>\n",
       "      <td>[time_body, repeat_body, fold_body, crossvalid...</td>\n",
       "      <td>[statist_title, report_title, repeat_title, cr...</td>\n",
       "      <td>[crossvalid_tags]</td>\n",
       "      <td>[time_body, repeat_body, fold_body, crossvalid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>487075</td>\n",
       "      <td>0</td>\n",
       "      <td>[dataset_body, mm_body, record_body, around_bo...</td>\n",
       "      <td>[binari_title, classif_title, imbalanc_title, ...</td>\n",
       "      <td>[unbalancedclassescalibr_tags]</td>\n",
       "      <td>[dataset_body, mm_body, record_body, around_bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>481670</td>\n",
       "      <td>2</td>\n",
       "      <td>[want_body, run_body, regress_body, one_body, ...</td>\n",
       "      <td>[best_title, summar_title, likert_title, data_...</td>\n",
       "      <td>[multipleregressionmissingdatalikertitemrespon...</td>\n",
       "      <td>[want_body, run_body, regress_body, one_body, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id  Score                                               Body  \\\n",
       "0  502641      1  [im_body, master_body, student_body, eec_body,...   \n",
       "1  477291      1  [know_body, good_body, question_body, found_bo...   \n",
       "2  448489      4  [time_body, repeat_body, fold_body, crossvalid...   \n",
       "3  487075      0  [dataset_body, mm_body, record_body, around_bo...   \n",
       "4  481670      2  [want_body, run_body, regress_body, one_body, ...   \n",
       "\n",
       "                                               Title  \\\n",
       "0  [pytorch_title, tutori_title, dqn_title, defin...   \n",
       "1           [random_title, walk_title, memori_title]   \n",
       "2  [statist_title, report_title, repeat_title, cr...   \n",
       "3  [binari_title, classif_title, imbalanc_title, ...   \n",
       "4  [best_title, summar_title, likert_title, data_...   \n",
       "\n",
       "                                                Tags  \\\n",
       "0  [machinelearningreinforcementlearningqlearn_tags]   \n",
       "1                   [probabilitylawoflargenumb_tags]   \n",
       "2                                  [crossvalid_tags]   \n",
       "3                     [unbalancedclassescalibr_tags]   \n",
       "4  [multipleregressionmissingdatalikertitemrespon...   \n",
       "\n",
       "                                                text  \n",
       "0  [im_body, master_body, student_body, eec_body,...  \n",
       "1  [know_body, good_body, question_body, found_bo...  \n",
       "2  [time_body, repeat_body, fold_body, crossvalid...  \n",
       "3  [dataset_body, mm_body, record_body, around_bo...  \n",
       "4  [want_body, run_body, regress_body, one_body, ...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4esXb6WGuvmo"
   },
   "source": [
    "### Q. Detokenize and convert to document term matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "JwhJ1pVBuvmo"
   },
   "outputs": [],
   "source": [
    "#Merge Three text column into one column and detokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text_train = df_train['text'].apply(TreebankWordDetokenizer().detokenize) #Todo: Detokenize your tokenized text data\n",
    "countvec_train = CountVectorizer(min_df=20)\n",
    "countvec = CountVectorizer(min_df=20) #Todo: Define your own CountVectorizer here\n",
    "sparse_dtm_train = countvec_train.fit_transform(text_train) #Todo: Fit and Transform your Countvectorizer and return sparse dtm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "VoO-e-00uvmo"
   },
   "outputs": [],
   "source": [
    "#Todo: Do same on the test set.\n",
    "text_test = df_test['text'].apply(TreebankWordDetokenizer().detokenize)\n",
    "sparse_dtm_test = countvec.fit_transform(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 340,
     "status": "ok",
     "timestamp": 1668128857596,
     "user": {
      "displayName": "Hyungki Im",
      "userId": "11059091126270115149"
     },
     "user_tz": 480
    },
    "id": "hiK01v4luvmp",
    "outputId": "f6521cdc-fd1a-446f-9a7c-eb03df04a5f2"
   },
   "outputs": [],
   "source": [
    "#Convert the sprase dtm to pandas DataFrame.\n",
    "dtm_train = pd.DataFrame(sparse_dtm_train.toarray(), columns=countvec_train.get_feature_names(), index=text_train.index)\n",
    "dtm_test = pd.DataFrame(sparse_dtm_test.toarray(), columns=countvec.get_feature_names(), index=text_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean the data from frequencies > 4\n",
    "# frequencies_train = dtm_train.sum().sort_values(ascending=False)\n",
    "# frequencies_test = dtm_test.sum().sort_values(ascending=False)\n",
    "# print(frequencies_train[frequencies_train > 10])\n",
    "# print(frequencies_test[frequencies_test > 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtm_train = frequencies_train > 10\n",
    "# dtm_test = frequencies_test > 10\n",
    "# print(dtm_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine the train_DTM and test_DTM together, using inner join to exclude features that only appear in one DTM.\n",
    "dtm = pd.concat([dtm_train, dtm_test], join = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate the new train_DTM and test_DTM from the combined DTM.\n",
    "new_dtm_train = dtm.iloc[0:dtm_train.shape[0]]\n",
    "new_dtm_test = dtm.iloc[dtm_train.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4241"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dtm_train.axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZVDS6druvmq"
   },
   "source": [
    "### Q. Change dependent variable to binary variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "DSQu_SPquvmq"
   },
   "outputs": [],
   "source": [
    "#Change 'Score' to a binary variable, which indicates whether the question is good or not.\n",
    "y_train = np.where(df_train['Score'] > 3, 1, 0) #Todo\n",
    "y_test = np.where(df_test['Score'] > 3, 1, 0) #Todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "kWhfmKlYuvmr"
   },
   "outputs": [],
   "source": [
    "#Add y_train and y_test to your data frame if it is needed. Drop unnecessary columns\n",
    "df_train['good_question'] = y_train\n",
    "df_test['good_question'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(columns=['good_question'], inplace=True)\n",
    "df_test.drop(columns=['good_question'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MiYas_A6JcS1"
   },
   "source": [
    "## Let's check our DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "WeTgM0ZMJqbI"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Score</th>\n",
       "      <th>Body</th>\n",
       "      <th>Title</th>\n",
       "      <th>Tags</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>502641</td>\n",
       "      <td>1</td>\n",
       "      <td>[im_body, master_body, student_body, eec_body,...</td>\n",
       "      <td>[pytorch_title, tutori_title, dqn_title, defin...</td>\n",
       "      <td>[machinelearningreinforcementlearningqlearn_tags]</td>\n",
       "      <td>[im_body, master_body, student_body, eec_body,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>477291</td>\n",
       "      <td>1</td>\n",
       "      <td>[know_body, good_body, question_body, found_bo...</td>\n",
       "      <td>[random_title, walk_title, memori_title]</td>\n",
       "      <td>[probabilitylawoflargenumb_tags]</td>\n",
       "      <td>[know_body, good_body, question_body, found_bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>448489</td>\n",
       "      <td>4</td>\n",
       "      <td>[time_body, repeat_body, fold_body, crossvalid...</td>\n",
       "      <td>[statist_title, report_title, repeat_title, cr...</td>\n",
       "      <td>[crossvalid_tags]</td>\n",
       "      <td>[time_body, repeat_body, fold_body, crossvalid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>487075</td>\n",
       "      <td>0</td>\n",
       "      <td>[dataset_body, mm_body, record_body, around_bo...</td>\n",
       "      <td>[binari_title, classif_title, imbalanc_title, ...</td>\n",
       "      <td>[unbalancedclassescalibr_tags]</td>\n",
       "      <td>[dataset_body, mm_body, record_body, around_bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>481670</td>\n",
       "      <td>2</td>\n",
       "      <td>[want_body, run_body, regress_body, one_body, ...</td>\n",
       "      <td>[best_title, summar_title, likert_title, data_...</td>\n",
       "      <td>[multipleregressionmissingdatalikertitemrespon...</td>\n",
       "      <td>[want_body, run_body, regress_body, one_body, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id  Score                                               Body  \\\n",
       "0  502641      1  [im_body, master_body, student_body, eec_body,...   \n",
       "1  477291      1  [know_body, good_body, question_body, found_bo...   \n",
       "2  448489      4  [time_body, repeat_body, fold_body, crossvalid...   \n",
       "3  487075      0  [dataset_body, mm_body, record_body, around_bo...   \n",
       "4  481670      2  [want_body, run_body, regress_body, one_body, ...   \n",
       "\n",
       "                                               Title  \\\n",
       "0  [pytorch_title, tutori_title, dqn_title, defin...   \n",
       "1           [random_title, walk_title, memori_title]   \n",
       "2  [statist_title, report_title, repeat_title, cr...   \n",
       "3  [binari_title, classif_title, imbalanc_title, ...   \n",
       "4  [best_title, summar_title, likert_title, data_...   \n",
       "\n",
       "                                                Tags  \\\n",
       "0  [machinelearningreinforcementlearningqlearn_tags]   \n",
       "1                   [probabilitylawoflargenumb_tags]   \n",
       "2                                  [crossvalid_tags]   \n",
       "3                     [unbalancedclassescalibr_tags]   \n",
       "4  [multipleregressionmissingdatalikertitemrespon...   \n",
       "\n",
       "                                                text  \n",
       "0  [im_body, master_body, student_body, eec_body,...  \n",
       "1  [know_body, good_body, question_body, found_bo...  \n",
       "2  [time_body, repeat_body, fold_body, crossvalid...  \n",
       "3  [dataset_body, mm_body, record_body, around_bo...  \n",
       "4  [want_body, run_body, regress_body, one_body, ...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVPT48NRuvms"
   },
   "source": [
    "## (b) Please read the instruction carefully in the pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "3yD6xH8PzyoZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2632"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = new_dtm_train\n",
    "x_test = new_dtm_test\n",
    "y_train = y_train.astype('int32')\n",
    "y_test = y_test.astype('int32')\n",
    "# y_train = df_train['good_question'].astype('int32')\n",
    "# x_train = dtm_train\n",
    "\n",
    "# y_test = df_train['good_question'].astype('int32')\n",
    "# x_test = dtm_test\n",
    "\n",
    "len(x_test.axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "grid_values = {'ccp_alpha': np.linspace(0, 0.005, 20)}\n",
    "\n",
    "dtc = DecisionTreeClassifier(random_state=88)\n",
    "dtc_cv = GridSearchCV(dtc, param_grid=grid_values, cv=5).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccp_alpha = dtc_cv.cv_results_['param_ccp_alpha'].data\n",
    "ACC_scores = dtc_cv.cv_results_['mean_test_score']\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.xlabel('ccp_alpha', fontsize=16)\n",
    "plt.ylabel('CV Accuracy', fontsize=16)\n",
    "plt.scatter(ccp_alpha, ACC_scores, s=3)\n",
    "plt.plot(ccp_alpha, ACC_scores, linewidth=3)\n",
    "plt.grid(True, which='both')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Best ccp_alpha', dtc_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "print('Node count =', dtc_cv.best_estimator_.tree_.node_count)\n",
    "plt.figure(figsize=(6,6))\n",
    "plot_tree(dtc_cv.best_estimator_, \n",
    "          feature_names=x_train.columns, \n",
    "          class_names=['0','1'], \n",
    "          filled=True,\n",
    "          impurity=False,\n",
    "          fontsize=12) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = dtc_cv.predict(x_test)\n",
    "dtc_cm = confusion_matrix(y_test, y_pred)\n",
    "dtc_acc = accuracy_score(y_test, y_pred)\n",
    "dtc_TPR = (dtc_cm.ravel()[3])/(dtc_cm.ravel()[2]+dtc_cm.ravel()[3])\n",
    "dtc_FPR = (dtc_cm.ravel()[1])/(dtc_cm.ravel()[0]+dtc_cm.ravel()[1])\n",
    "\n",
    "print(dtc_cm)\n",
    "print(dtc_acc)\n",
    "print(dtc_TPR)\n",
    "print(dtc_FPR)\n",
    "\n",
    "#dtc_PRE\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "# print (\"Confusion Matrix: \\n\", cm)\n",
    "# print (\"\\nAccuracy:\", accuracy_score(y_test, y_pred))\n",
    "# print (\"TPR:\", (cm.ravel()[3])/(cm.ravel()[2]+cm.ravel()[3]))\n",
    "# print (\"FPR:\", (cm.ravel()[1])/(cm.ravel()[0]+cm.ravel()[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=88)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(random_state=88)\n",
    "logreg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7602  166]\n",
      " [ 446   35]]\n",
      "0.9258091889926051\n",
      "0.07276507276507277\n",
      "0.0213697219361483\n"
     ]
    }
   ],
   "source": [
    "# y_prob = logreg.predict(x_test)\n",
    "y_prob = logreg.predict_proba(x_test)\n",
    "# Convert numpy array to Pandas DataFrame\n",
    "y_test_df = pd.DataFrame(y_prob)\n",
    "y_pred = pd.Series([1 if x > 0.5 else 0 for x in y_prob[:,1]], index=y_test_df.index)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "log_cm = confusion_matrix(y_test, y_pred)\n",
    "log_acc = accuracy_score(y_test, y_pred)\n",
    "log_TPR = (log_cm.ravel()[3])/(log_cm.ravel()[2]+log_cm.ravel()[3])\n",
    "log_FPR = (log_cm.ravel()[1])/(log_cm.ravel()[0]+log_cm.ravel()[1])\n",
    "\n",
    "print(log_cm)\n",
    "print(log_acc)\n",
    "print(log_TPR)\n",
    "print(log_FPR)\n",
    "#log_PRE\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "# print (\"Confusion Matrix: \\n\", cm)\n",
    "# print (\"\\nAccuracy:\", accuracy_score(y_test, y_pred))\n",
    "# print (\"TPR:\", (cm.ravel()[3])/(cm.ravel()[2]+cm.ravel()[3]))\n",
    "# print (\"FPR:\", (cm.ravel()[1])/(cm.ravel()[0]+cm.ravel()[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7444  324]\n",
      " [ 418   63]]\n",
      "0.9100497029943023\n",
      "0.13097713097713098\n",
      "0.04170957775489186\n"
     ]
    }
   ],
   "source": [
    "y_pred = lda.predict(x_test)\n",
    "lda_cm = confusion_matrix(y_test, y_pred)\n",
    "lda_acc = accuracy_score(y_test, y_pred)\n",
    "lda_TPR = (lda_cm.ravel()[3])/(lda_cm.ravel()[2]+lda_cm.ravel()[3])\n",
    "lda_FPR = (lda_cm.ravel()[1])/(lda_cm.ravel()[0]+lda_cm.ravel()[1])\n",
    "#lda_PRE\n",
    "\n",
    "print(lda_cm)\n",
    "print(lda_acc)\n",
    "print(lda_TPR)\n",
    "print(lda_FPR)\n",
    "\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "# print (\"Confusion Matrix: \\n\", cm)\n",
    "# print (\"\\nAccuracy:\", accuracy_score(y_test, y_pred))\n",
    "# print (\"TPR:\", (cm.ravel()[3])/(cm.ravel()[2]+cm.ravel()[3]))\n",
    "# print (\"FPR:\", (cm.ravel()[1])/(cm.ravel()[0]+cm.ravel()[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Instantiate the Random Forest classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=88)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "rf_clf.fit(x_train, y_train)\n",
    "\n",
    "# Predict the labels of the test data\n",
    "y_pred = rf_clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7766    2]\n",
      " [ 481    0]]\n",
      "0.9414474481755364\n",
      "0.0\n",
      "0.00025746652935118434\n"
     ]
    }
   ],
   "source": [
    "rf_cm = confusion_matrix(y_test, y_pred)\n",
    "rf_acc = accuracy_score(y_test, y_pred)\n",
    "rf_TPR = (rf_cm.ravel()[3])/(rf_cm.ravel()[2]+rf_cm.ravel()[3])\n",
    "rf_FPR = (rf_cm.ravel()[1])/(rf_cm.ravel()[0]+rf_cm.ravel()[1])\n",
    "\n",
    "print(rf_cm)\n",
    "print(rf_acc)\n",
    "print(rf_TPR)\n",
    "print(rf_FPR)\n",
    "\n",
    "#rf_PRE\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "# print (\"Confusion Matrix: \\n\", cm)\n",
    "# print (\"\\nAccuracy:\", accuracy_score(y_test, y_pred))\n",
    "# print (\"TPR:\", (cm.ravel()[3])/(cm.ravel()[2]+cm.ravel()[3]))\n",
    "# print (\"FPR:\", (cm.ravel()[1])/(cm.ravel()[0]+cm.ravel()[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Instantiate a dummy classifier with the strategy \"most frequent\"\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "dummy_clf.fit(x_train, y_train)\n",
    "\n",
    "# Predict the labels of the test data\n",
    "y_pred = dummy_clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7768    0]\n",
      " [ 481    0]]\n",
      "0.9416899018062795\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "baseline_cm = confusion_matrix(y_test, y_pred)\n",
    "baseline_acc = accuracy_score(y_test, y_pred)\n",
    "baseline_TPR = (baseline_cm.ravel()[3])/(baseline_cm.ravel()[2]+baseline_cm.ravel()[3])\n",
    "baseline_FPR = (baseline_cm.ravel()[1])/(baseline_cm.ravel()[0]+baseline_cm.ravel()[1])\n",
    "\n",
    "print(baseline_cm)\n",
    "print(baseline_acc)\n",
    "print(baseline_TPR)\n",
    "print(baseline_FPR)\n",
    "\n",
    "#baseline_PRE\n",
    "# print (\"Confusion Matrix: \\n\", cm)\n",
    "# print (\"\\nAccuracy:\", accuracy_score(y_test, y_pred))\n",
    "# print (\"TPR:\", (cm.ravel()[3])/(cm.ravel()[2]+cm.ravel()[3]))\n",
    "# print (\"FPR:\", (cm.ravel()[1])/(cm.ravel()[0]+cm.ravel()[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>TPR</th>\n",
       "      <th>FPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>0.941690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.925809</td>\n",
       "      <td>0.072765</td>\n",
       "      <td>0.021370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest with CV</th>\n",
       "      <td>0.941447</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear Discriminant Analysis</th>\n",
       "      <td>0.910050</td>\n",
       "      <td>0.130977</td>\n",
       "      <td>0.041710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Accuracy       TPR       FPR\n",
       "Baseline                      0.941690  0.000000  0.000000\n",
       "Logistic Regression           0.925809  0.072765  0.021370\n",
       "Random Forest with CV         0.941447  0.000000  0.000257\n",
       "Linear Discriminant Analysis  0.910050  0.130977  0.041710"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create Comparison Table\n",
    "#These lines are provided for you to help construct a comparison table.\n",
    "#It is not requred to follow this format. + You need to find ACC, TPR, FPR, PRE for each model that you choose.\n",
    "comparison_data = {'Baseline':[baseline_acc,baseline_TPR,baseline_FPR],\n",
    "                   'Logistic Regression':[log_acc,log_TPR,log_FPR],\n",
    "                   'Random Forest with CV':[rf_acc,rf_TPR, rf_FPR],\n",
    "                   'Linear Discriminant Analysis':[lda_acc,lda_TPR, lda_FPR]}\n",
    "#                   'Decision Tree Classifier':[dtc_acc,dtc_TPR,dtc_FPR],\n",
    "\n",
    "comparison_table = pd.DataFrame(data=comparison_data, index=['Accuracy', 'TPR', 'FPR']).transpose()\n",
    "comparison_table.style.set_properties(**{'font-size': '12pt',}).set_table_styles([{'selector': 'th', 'props': [('font-size', '10pt')]}])\n",
    "comparison_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useing bootstrap to asses the performance of the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B=1000\n",
    "\n",
    "output_array=np.zeros([B, 3])\n",
    "for bs_iter in range(B):\n",
    "    bs_index = np.random.choice(x_test.index, len(x_test.index), replace=True)\n",
    "    bs_X = x_test.loc[bs_index]\n",
    "    bs_y = y_test.loc[bs_index]\n",
    "    bs_y_pred = logreg.predict(bs_X)\n",
    "    cm = confusion_matrix(bs_y,bs_y_pred)\n",
    "    ACC = (cm[0][0]+cm[1][1])/len(x_test.index)\n",
    "    TPR = cm[1][1]/sum(cm[1])\n",
    "    FPR = cm[0][1]/sum(cm[0])\n",
    "    output_array[bs_iter,:] = ACC, TPR, FPR\n",
    "bs_output = pd.DataFrame(output_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=3,figsize=(12,4))\n",
    "axs[0].set_xlabel('ACC of model on bootstrapped set')\n",
    "axs[1].set_xlabel('TPR of model on bootstrapped set')\n",
    "axs[2].set_xlabel('FPR of model on bootstrapped set')\n",
    "axs[0].set_ylabel('Count')\n",
    "axs[0].hist(bs_output.iloc[:,0], bins=20, edgecolor = 'orange', linewidth=2, color = \"yellow\")\n",
    "axs[1].hist(bs_output.iloc[:,1], bins=20, edgecolor = 'orange', linewidth=2, color = \"yellow\")\n",
    "axs[2].hist(bs_output.iloc[:,2], bins=20, edgecolor = 'orange', linewidth=2, color = \"yellow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-O3EsbQGU0uX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mjsc7lf6zOms"
   },
   "source": [
    "\n",
    "## Report details of your training procedures and final comparisons on the test set in this cell. Use your best judgment to choose a final model and explain your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLa-KDajuvmz"
   },
   "source": [
    "## Report Bootstrap Analysis in this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JrCM_96Quvm2"
   },
   "source": [
    "### (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uh2mvNrrTFBU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [
    {
     "file_id": "1AHr2NIU61Oco-PfG10o3gAYXLrsyJ8Sv",
     "timestamp": 1668122684758
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
