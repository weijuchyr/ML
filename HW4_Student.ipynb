{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gl0OH3XGTXe1"
   },
   "source": [
    "# HW5 Skeleton Code\n",
    "Please note that this skeleton code is provided to help you with homework.\n",
    "Full description of each question can be found on HW5.pdf, so please read instruction of each question carefully. There might be some questions that is not presented in this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1QYqL7lvuvmK"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "awc_PHjUuvmR"
   },
   "source": [
    "## Q. Changing HTML Text to Plain Text\n",
    "\n",
    "The Python library <b>BeautifulSoup</b> is useful for dealing with html text. In order to use this library, you will need to install it first by running the following command: \n",
    " <b>conda install beautifulsoup4</b> \n",
    " in the terminal.\n",
    " <br> In the code, you can import it by running the following line: \n",
    "<br> \n",
    "  <b>from bs4 import BeautifulSoup </b>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JaHFZFOuuvmU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        <machine-learning><reinforcement-learning><q-l...\n",
       "1                      <probability><law-of-large-numbers>\n",
       "2                                       <cross-validation>\n",
       "3                        <unbalanced-classes><calibration>\n",
       "4        <multiple-regression><missing-data><likert><it...\n",
       "                               ...                        \n",
       "19242    <machine-learning><data-imputation><recommende...\n",
       "19243    <regression><modeling><measurement-error><erro...\n",
       "19244                               <poisson-distribution>\n",
       "19245    <machine-learning><mathematical-statistics><cu...\n",
       "19246                                  <r><bioinformatics>\n",
       "Name: Tags, Length: 19247, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  #Read our data file\n",
    "df_train = pd.read_csv(r'stack_stats_2023_train.csv') #Todo\n",
    "df_test = pd.read_csv(r'stack_stats_2023_test.csv') #Todo\n",
    "\n",
    "df_train['Tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jzyC4JZpuvmW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         \n",
       "1         \n",
       "2         \n",
       "3         \n",
       "4         \n",
       "        ..\n",
       "19242     \n",
       "19243     \n",
       "19244     \n",
       "19245     \n",
       "19246     \n",
       "Name: Tags, Length: 19247, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cleaning 'Body'\n",
    "#Change HTML Text to Plain text using get_text() function from BeautifulSoup\n",
    "#If you are not familiar with the apply method, please check discussion week 10 lecture and code.\n",
    "\n",
    "#df_train['Body'] = df_train['Body'].apply(lambda x: soup.get_text(x))\n",
    "#Manually cleaned up newline tag \\n and tab tag \\t.\n",
    "df_train['Body'] = df_train['Body'].apply(lambda x: BeautifulSoup(x).get_text().replace('/n', '')) #Todo\n",
    "#If you need any other cleaning process, please uncomment the below.\n",
    "#df_train['Body'] = df_train['Body'].apply(lambda ) #Todo\n",
    "\n",
    "#Cleaning Tags\n",
    "#This would be somewhat similar to the above.\n",
    "#df_train['Tags'] = df_train['Tags'].apply(lambda x: soup.get_text(x)) #Todo\n",
    "#Manually cleaned up newline tag \\n and tab tag \\t.\n",
    "df_train['Tags'] = df_train['Tags'].apply(lambda x: BeautifulSoup(x).get_text().replace('/n', ''))\n",
    "\n",
    "\n",
    "#Todo: Repeat the same process for test dataset \n",
    "df_test['Body'] = df_test['Body'].apply(lambda x: BeautifulSoup(x).get_text().replace('/n', ''))\n",
    "df_test['Tags'] = df_test['Tags'].apply(lambda x: BeautifulSoup(x).get_text().replace('/n', ''))\n",
    "\n",
    "\n",
    "df_train['Tags']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNS-3DkCuvmb"
   },
   "source": [
    "## Q. Basic Text Cleaning and Merging into a single Text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "031sBi5Duvmc"
   },
   "source": [
    "### Change to Lower Case, Remove puncuation, digits, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "t-YUYKlvuvmd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         \n",
       "1         \n",
       "2         \n",
       "3         \n",
       "4         \n",
       "        ..\n",
       "19242     \n",
       "19243     \n",
       "19244     \n",
       "19245     \n",
       "19246     \n",
       "Name: Tags, Length: 19247, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Change to Lowercase\n",
    "\n",
    "df_train[['Body','Title','Tags']] = df_train[['Body','Title','Tags']].applymap(lambda x: x.lower()) #Todo, do you see why we used applymap instead of apply in this case? \n",
    "df_test[['Body','Title','Tags']] = df_test[['Body','Title','Tags']].applymap(lambda x: x.lower()) #Todo\n",
    "\n",
    "df_train['Tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Bbcp_w_wuvme"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         \n",
       "1         \n",
       "2         \n",
       "3         \n",
       "4         \n",
       "        ..\n",
       "19242     \n",
       "19243     \n",
       "19244     \n",
       "19245     \n",
       "19246     \n",
       "Name: Tags, Length: 19247, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove Punctations \n",
    "from string import punctuation\n",
    "\n",
    "#You can get this function from our discussion session code. However, we leave it as a blank for a practice.\n",
    "def remove_punctuation(document):\n",
    "    \n",
    "    no_punct = ''.join([character for character in document if character not in punctuation])#Todo\n",
    "\n",
    "    return no_punct\n",
    "\n",
    "df_train['Body'] = df_train['Body'].apply(remove_punctuation)\n",
    "df_train['Title'] = df_train['Title'].apply(remove_punctuation)\n",
    "df_train['Tags'] = df_train['Tags'].apply(remove_punctuation)\n",
    "df_test['Body'] = df_test['Body'].apply(remove_punctuation)\n",
    "df_test['Title'] = df_test['Title'].apply(remove_punctuation)\n",
    "df_test['Tags'] = df_test['Tags'].apply(remove_punctuation)\n",
    "#df_train[['Body','Title','Tags']] = df_train[['Body','Title','Tags']].apply(remove_punctuation)#Todo \n",
    "#df_test[['Body','Title','Tags']] = df_test[['Body','Title','Tags']].apply(remove_punctuation)#Todo\n",
    "\n",
    "df_train['Tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yNUiymSJuvmh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         \n",
       "1         \n",
       "2         \n",
       "3         \n",
       "4         \n",
       "        ..\n",
       "19242     \n",
       "19243     \n",
       "19244     \n",
       "19245     \n",
       "19246     \n",
       "Name: Tags, Length: 19247, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove Digits \n",
    "\n",
    "def remove_digit(document): \n",
    "    \n",
    "    no_digit = ''.join([character for character in document if not character.isdigit()])#Todo\n",
    "              \n",
    "    return no_digit\n",
    "\n",
    "df_train['Body'] = df_train['Body'].apply(remove_digit)\n",
    "df_train['Title'] = df_train['Title'].apply(remove_digit)\n",
    "df_train['Tags'] = df_train['Tags'].apply(remove_digit)\n",
    "df_test['Body'] = df_test['Body'].apply(remove_digit)\n",
    "df_test['Title'] = df_test['Title'].apply(remove_digit)\n",
    "df_test['Tags'] = df_test['Tags'].apply(remove_digit)\n",
    "#df_train[['Body','Title','Tags']] = df_train[['Body','Title','Tags']].apply(remove_digit)#Todo \n",
    "#df_test[['Body','Title','Tags']] = df_test[['Body','Title','Tags']].apply(remove_digit)#Todo\n",
    "\n",
    "df_train['Tags']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PAsCQQPQuvmi"
   },
   "source": [
    "### Tokenization and Remove Stopwords and do stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25821,
     "status": "ok",
     "timestamp": 1668123057199,
     "user": {
      "displayName": "Hyungki Im",
      "userId": "11059091126270115149"
     },
     "user_tz": 480
    },
    "id": "gOrVZxXQuvmj",
    "outputId": "07dd90e0-e5ca-4634-f43f-f379fe3a0787"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        []\n",
       "1        []\n",
       "2        []\n",
       "3        []\n",
       "4        []\n",
       "         ..\n",
       "19242    []\n",
       "19243    []\n",
       "19244    []\n",
       "19245    []\n",
       "19246    []\n",
       "Name: Tags, Length: 19247, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "df_train['Body'] = df_train['Body'].apply(word_tokenize)\n",
    "df_train['Title'] = df_train['Title'].apply(word_tokenize)\n",
    "df_train['Tags'] = df_train['Tags'].apply(word_tokenize)\n",
    "df_test['Body'] = df_test['Body'].apply(word_tokenize)\n",
    "df_test['Title'] = df_test['Title'].apply(word_tokenize)\n",
    "df_test['Tags'] = df_test['Tags'].apply(word_tokenize)\n",
    "#df_train[['Body','Title','Tags']] = df_train[['Body','Title','Tags']].apply(word_tokenize)#Todo \n",
    "#df_test[['Body','Title','Tags']] = df_test[['Body','Title','Tags']].apply(word_tokenize)#Todo\n",
    "\n",
    "df_train['Tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2684,
     "status": "ok",
     "timestamp": 1668123197885,
     "user": {
      "displayName": "Hyungki Im",
      "userId": "11059091126270115149"
     },
     "user_tz": 480
    },
    "id": "82DJNdV1uvmj",
    "outputId": "8f8148ae-7551-43fc-d3d3-2bc8d8202d15"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        []\n",
       "1        []\n",
       "2        []\n",
       "3        []\n",
       "4        []\n",
       "         ..\n",
       "19242    []\n",
       "19243    []\n",
       "19244    []\n",
       "19245    []\n",
       "19246    []\n",
       "Name: Tags, Length: 19247, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove Stopwords\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(document):\n",
    "    \n",
    "    words = [word for word in document if not word in stop_words]#Todo\n",
    "    \n",
    "    return words\n",
    "\n",
    "df_train['Body'] = df_train['Body'].apply(remove_stopwords)\n",
    "df_train['Title'] = df_train['Title'].apply(remove_stopwords)\n",
    "df_train['Tags'] = df_train['Tags'].apply(remove_stopwords)\n",
    "df_test['Body'] = df_test['Body'].apply(remove_stopwords)\n",
    "df_test['Title'] = df_test['Title'].apply(remove_stopwords)\n",
    "df_test['Tags'] = df_test['Tags'].apply(remove_stopwords)\n",
    "#df_train[['Body','Title','Tags']] = df_train[['Body','Title','Tags']].apply(lambda s: s.remove_stopwords() if type(s)==str else s)#Todo\n",
    "#df_test[['Body','Title','Tags']] = df_test[['Body','Title','Tags']].apply(lambda s: s.remove_stopwords() if type(s)==str else s)#Todo\n",
    "\n",
    "df_train['Tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "54S-irnRuvmk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        []\n",
       "1        []\n",
       "2        []\n",
       "3        []\n",
       "4        []\n",
       "         ..\n",
       "19242    []\n",
       "19243    []\n",
       "19244    []\n",
       "19245    []\n",
       "19246    []\n",
       "Name: Tags, Length: 19247, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We use porter stemming \n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def stemmer(document):\n",
    "    \n",
    "    stemmed_document = [porter.stem(word) for word in document] #Todo\n",
    "    \n",
    "    return stemmed_document\n",
    "\n",
    "df_train['Body'] = df_train['Body'].apply(stemmer)\n",
    "df_train['Title'] = df_train['Title'].apply(stemmer)\n",
    "df_train['Tags'] = df_train['Tags'].apply(stemmer)\n",
    "df_test['Body'] = df_test['Body'].apply(stemmer)\n",
    "df_test['Title'] = df_test['Title'].apply(stemmer)\n",
    "df_test['Tags'] = df_test['Tags'].apply(stemmer)\n",
    "#df_train[['Body','Title','Tags']] = df_train[['Body','Title','Tags']].apply(lambda s: s.stemmer() if type(s)==str else s) #Todo\n",
    "#df_test[['Body','Title','Tags']] = df_test[['Body','Title','Tags']].apply(lambda s: s.stemmer() if type(s)==str else s) #Todo\n",
    "\n",
    "df_train['Tags']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2sfBcaMLTok"
   },
   "source": [
    "## Let's Check our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "r6V33S7pLS76"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Score</th>\n",
       "      <th>Body</th>\n",
       "      <th>Title</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>502641</td>\n",
       "      <td>1</td>\n",
       "      <td>[im, master, student, eec, work, way, toward, ...</td>\n",
       "      <td>[pytorch, tutori, dqn, defin, state, differ]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>477291</td>\n",
       "      <td>1</td>\n",
       "      <td>[know, good, question, found, answer, anywher,...</td>\n",
       "      <td>[random, walk, memori]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>448489</td>\n",
       "      <td>4</td>\n",
       "      <td>[time, repeat, fold, crossvalid, want, report,...</td>\n",
       "      <td>[statist, report, repeat, crossvalid]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>487075</td>\n",
       "      <td>0</td>\n",
       "      <td>[dataset, mm, record, around, featur, class, i...</td>\n",
       "      <td>[binari, classif, imbalanc, data, odd, calibr,...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>481670</td>\n",
       "      <td>2</td>\n",
       "      <td>[want, run, regress, one, explanatori, variabl...</td>\n",
       "      <td>[best, summar, likert, data, use, independ, va...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id  Score                                               Body  \\\n",
       "0  502641      1  [im, master, student, eec, work, way, toward, ...   \n",
       "1  477291      1  [know, good, question, found, answer, anywher,...   \n",
       "2  448489      4  [time, repeat, fold, crossvalid, want, report,...   \n",
       "3  487075      0  [dataset, mm, record, around, featur, class, i...   \n",
       "4  481670      2  [want, run, regress, one, explanatori, variabl...   \n",
       "\n",
       "                                               Title Tags  \n",
       "0       [pytorch, tutori, dqn, defin, state, differ]   []  \n",
       "1                             [random, walk, memori]   []  \n",
       "2              [statist, report, repeat, crossvalid]   []  \n",
       "3  [binari, classif, imbalanc, data, odd, calibr,...   []  \n",
       "4  [best, summar, likert, data, use, independ, va...   []  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hFgV2VL0uvml"
   },
   "source": [
    "### Q. Treat Three text data independently and merge into one column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "VCB-voc_uvmm"
   },
   "outputs": [],
   "source": [
    "#Treat Three types of data independently\n",
    "#let's define functions that will help this operation\n",
    "\n",
    "def add_body(document):\n",
    "    \n",
    "    added_document = document.add('Body') #Todo\n",
    "    \n",
    "    return added_document\n",
    "\n",
    "def add_title(document):\n",
    "    \n",
    "    added_document = document.add('Title') #Todo\n",
    "    \n",
    "    return added_document\n",
    "\n",
    "def add_tags(document):\n",
    "    \n",
    "    added_document = document.add('Tags') #Todo\n",
    "    \n",
    "    return added_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "UWy0BGVHuvmm"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'add'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-40260a215b19>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#df_test['Tags'] = df_test['Tags'].apply(lambda s: s.add_tags() if type(s)==str else s)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Body'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Body'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madd_body\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Title'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madd_title\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Tags'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Tags'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madd_tags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   4136\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4137\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4138\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4140\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-fc869360ed05>\u001b[0m in \u001b[0;36madd_body\u001b[1;34m(document)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0madd_body\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0madded_document\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Body'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Todo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0madded_document\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'add'"
     ]
    }
   ],
   "source": [
    "#df_train['Body'] = df_train['Body'].apply(lambda s: s.add_body() if type(s)==str else s)\n",
    "#df_train['Title'] = df_train['Title'].apply(lambda s: s.add_title() if type(s)==str else s)\n",
    "#df_train['Tags'] = df_train['Tags'].apply(lambda s: s.add_tags() if type(s)==str else s)\n",
    "\n",
    "#df_test['Body'] = df_test['Body'].apply(lambda s: s.add_body() if type(s)==str else s)\n",
    "#df_test['Title'] = df_test['Title'].apply(lambda s: s.add_title() if type(s)==str else s)\n",
    "#df_test['Tags'] = df_test['Tags'].apply(lambda s: s.add_tags() if type(s)==str else s)\n",
    "\n",
    "df_train['Body'] = df_train['Body'].apply(add_body)\n",
    "df_train['Title'] = df_train['Title'].apply(add_title)\n",
    "df_train['Tags'] = df_train['Tags'].apply(add_tags)\n",
    "\n",
    "df_test['Body'] = df_test['Body'].apply(add_body)\n",
    "df_test['Title'] = df_test['Title'].apply(add_title)\n",
    "df_test['Tags'] = df_test['Tags'].apply(add_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "kgcE1Vcnuvmn"
   },
   "outputs": [],
   "source": [
    "#Now we need to merge all those 3 columns into a single column. Implement this below.\n",
    "#df_train['text'] = df_train[['Body','Title','Tags']].apply(lambda x: pd.DataFrame.join()) #Todo\n",
    "train_files = [train_Body, train_Title, train_Tags]\n",
    "test_files = [test_Body, test_Title, test_Tags]\n",
    "df_train = pd.concat(train_files, axis = 1, join = 'inner')\n",
    "df_test = pd.concat(test_files, axis = 1, join = 'inner') #Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hq2PYhnmLhDi"
   },
   "source": [
    "## Let's check our DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rtYP_Lu3LjIb"
   },
   "outputs": [],
   "source": [
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4esXb6WGuvmo"
   },
   "source": [
    "### Q. Detokenize and convert to document term matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JwhJ1pVBuvmo"
   },
   "outputs": [],
   "source": [
    "#Merge Three text column into one column and detokenize\n",
    "\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text_train = df_train['text'].apply() #Todo: Detokenize your tokenized text data\n",
    "countvec_train = #Todo: Define your own CountVectorizer here\n",
    "sparse_dtm_train = #Todo: Fit and Transform your Countvectorizer and return sparse dtm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VoO-e-00uvmo"
   },
   "outputs": [],
   "source": [
    "#Todo: Do same on the test set.\n",
    "text_test = df_test['text'].apply()\n",
    "sparse_dtm_test = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 340,
     "status": "ok",
     "timestamp": 1668128857596,
     "user": {
      "displayName": "Hyungki Im",
      "userId": "11059091126270115149"
     },
     "user_tz": 480
    },
    "id": "hiK01v4luvmp",
    "outputId": "f6521cdc-fd1a-446f-9a7c-eb03df04a5f2"
   },
   "outputs": [],
   "source": [
    "#Convert the sprase dtm to pandas DataFrame.\n",
    "dtm_train = #Todo\n",
    "dtm_test = #Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZVDS6druvmq"
   },
   "source": [
    "### Q. Change dependent variable to binary variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DSQu_SPquvmq"
   },
   "outputs": [],
   "source": [
    "#Change 'Score' to a binary variable, which indicates whether the question is good or not.\n",
    "y_train = #Todo\n",
    "y_test = #Todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kWhfmKlYuvmr"
   },
   "outputs": [],
   "source": [
    "#Add y_train and y_test to your data frame if it is needed. Drop unnecessary columns\n",
    "df_train[''] = y_train\n",
    "df_test[''] = y_test\n",
    "df_train.drop(columns = [], inplace = True)\n",
    "df_test.drop(columns = [], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MiYas_A6JcS1"
   },
   "source": [
    "## Let's check our DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WeTgM0ZMJqbI"
   },
   "outputs": [],
   "source": [
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVPT48NRuvms"
   },
   "source": [
    "## (b) Please read the instruction carefully in the pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3yD6xH8PzyoZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-O3EsbQGU0uX"
   },
   "outputs": [],
   "source": [
    "#Create Comparison Table\n",
    "#These lines are provided for you to help construct a comparison table.\n",
    "#It is not requred to follow this format. + You need to find ACC, TPR, FPR, PRE for each model that you choose.\n",
    "comparison_data = {'Baseline':[baseline_acc,baseline_TPR,baseline_FPR, baseline_PRE],\n",
    "                   'Logistic Regression':[log_acc,log_TPR,log_FPR, log_PRE],\n",
    "                   'Decision Tree Classifier':[dtc_acc,dtc_TPR,dtc_FPR,dtc_PRE],\n",
    "                   'Random Forest with CV':[rf_acc,rf_TPR, rf_FPR,rf_PRE],\n",
    "                  'Linear Discriminant Analysis':[lda_acc,lda_TPR, lda_FPR,lda_PRE]}\n",
    "\n",
    "comparison_table = pd.DataFrame(data=comparison_data, index=['Accuracy', 'TPR', 'FPR','PRE']).transpose()\n",
    "comparison_table.style.set_properties(**{'font-size': '12pt',}).set_table_styles([{'selector': 'th', 'props': [('font-size', '10pt')]}])\n",
    "comparison_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mjsc7lf6zOms"
   },
   "source": [
    "\n",
    "## Report details of your training procedures and final comparisons on the test set in this cell. Use your best judgment to choose a final model and explain your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLa-KDajuvmz"
   },
   "source": [
    "## Report Bootstrap Analysis in this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JrCM_96Quvm2"
   },
   "source": [
    "### (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uh2mvNrrTFBU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [
    {
     "file_id": "1AHr2NIU61Oco-PfG10o3gAYXLrsyJ8Sv",
     "timestamp": 1668122684758
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
